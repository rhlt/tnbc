{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0635643-b03b-4821-9450-41b04b7ecc84",
   "metadata": {},
   "source": [
    "### Choose model and featureset to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b644131-76bc-4de4-9ba2-00a97baafbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"..\\Model\\DataHelpers.ipynb\"\n",
    "\n",
    "# Can be replaced with desired variant for different feature sets\n",
    "GENE_FILE_VARIANT = 'lasso'\n",
    "\n",
    "MODEL_VARIANT_SVM = ModelVariant.SVM\n",
    "MODEL_VARIANT_RF = ModelVariant.RF\n",
    "MODEL_VARIANT_LG = ModelVariant.LG\n",
    "\n",
    "FILE_PATH_LG_OUTPUT   = f\"../Data/model_output_lasso_validation_LogisticRegression.csv\"\n",
    "FILE_PATH_RF_OUTPUT   = f\"../Data/model_output_lasso_validation_RandomForestClassifier.csv\"\n",
    "FILE_PATH_SVM_OUTPUT  = f\"../Data/model_output_lasso_validation_SVC.csv\"\n",
    "\n",
    "FILE_PATH_LG_METRICS  = f\"../Data/model_metrics_lasso_validation_LogisticRegression.csv\"\n",
    "FILE_PATH_RF_METRICS  = f\"../Data/model_metrics_lasso_validation_RandomForestClassifier.csv\"\n",
    "FILE_PATH_SVM_METRICS = f\"../Data/model_metrics_lasso_validation_SVC.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import to_hex\n",
    "\n",
    "# Output files to load variants for\n",
    "variants = {\n",
    "    \"Random Forest\": FILE_PATH_RF_OUTPUT,\n",
    "    \"Support Vector Machine\": FILE_PATH_SVM_OUTPUT,\n",
    "    \"Logistic Regression\": FILE_PATH_LG_OUTPUT\n",
    "}\n",
    "# Cross-validation metrics files to load variants for\n",
    "variantMetrics = {\n",
    "    \"Random Forest\": FILE_PATH_RF_METRICS,\n",
    "    \"Support Vector Machine\": FILE_PATH_SVM_METRICS,\n",
    "    \"Logistic Regression\": FILE_PATH_LG_METRICS\n",
    "}\n",
    "baseColors = [] # To override the first couple of base colors, add them here (removed because unnecessary)\n",
    "getColors = lambda n: ([ # Lambda to generate as many base colors as needed\n",
    "    to_hex(plt.get_cmap('tab10')((i + len(baseColors)) % plt.get_cmap('tab10').N))\n",
    "    for i in range(max(0, n - len(baseColors)))\n",
    "])[:n]\n",
    "\n",
    "# Definitions\n",
    "evalPrecision = 'precision'\n",
    "evalRecall = 'recall'\n",
    "evalF1score = 'f1-score'\n",
    "\n",
    "evalTargetN = 'nTNBC'\n",
    "evalTarget = 'TNBC'\n",
    "\n",
    "targetNames = [evalTargetN, evalTarget]\n",
    "classLabels = ['Precision', 'Recall', 'F1-score']\n",
    "models = variants.keys()\n",
    "\n",
    "notTNBCData = {}\n",
    "TNBCData = {}\n",
    "accuracyData = {}\n",
    "confusionMatrixes = {}\n",
    "\n",
    "# Read evaluation data for all files\n",
    "for model, file in variants.items():\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    y_test = df['y_validation']\n",
    "    y_pred = df['y_pred']\n",
    "    \n",
    "    classification = classification_report(y_test, y_pred, output_dict=True, target_names=targetNames, zero_division=0)\n",
    "    confusionMatrix = confusion_matrix(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    notTNBCData[model] = (classification[evalTargetN][evalPrecision]*100 , classification[evalTargetN][evalRecall]*100 , classification[evalTargetN][evalF1score]*100 )\n",
    "    TNBCData[model] = (classification[evalTarget][evalPrecision]*100 , classification[evalTarget][evalRecall]*100 , classification[evalTarget][evalF1score]*100 )\n",
    "    accuracyData[model] = accuracy*100\n",
    "    confusionMatrixes[model] = confusionMatrix\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define figure size (flexible to permit more than 3 figures at once, and to decide the number of columns to use)\n",
    "ncols = 3 # Minimum three, for Accuracy, Metrics TNBC, Metrics nTNBC in the first row\n",
    "nrows = ((len(accuracyData) + (ncols - 1)) // ncols) + 1\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, layout='constrained', figsize=[4*ncols, 3*nrows])\n",
    "colors = getColors(len(accuracyData))\n",
    "      \n",
    "# == Accuracy\n",
    "ax[0,0].set_xlim(0,100) \n",
    "rects = ax[0,0].barh(y=accuracyData.keys(), width=accuracyData.values(), color=colors)\n",
    "ax[0,0].bar_label(rects, padding=3, fmt='%.2f%%')\n",
    "ax[0,0].set_ylabel('Accuracy (%)', rotation=0, horizontalalignment='left')\n",
    "ax[0,0].yaxis.set_label_coords(-0.37, 0.8)\n",
    "ax[0,0].set_yticklabels('')\n",
    "ax[0,0].invert_yaxis()\n",
    "\n",
    "# == TNBC\n",
    "width = 0.25\n",
    "multiplier = 0\n",
    "ax[0,1].set_xlim(0, 100)\n",
    "y_pos = np.arange(3)\n",
    "for attribute, measurement in TNBCData.items():\n",
    "    offset = width * multiplier\n",
    "    rectsTNBC = ax[0,1].barh(y_pos + offset, measurement, width, label=attribute, color=colors[multiplier])\n",
    "    ax[0,1].bar_label(rectsTNBC, padding=3, fmt='%.2f%%')\n",
    "    multiplier += 1\n",
    "\n",
    "ax[0,1].set_yticks(y_pos)\n",
    "ax[0,1].set_yticklabels(classLabels)\n",
    "ax[0,1].invert_yaxis()\n",
    "ax[0,1].set_xlabel('Metrics TNBC')\n",
    "\n",
    "# == tNBC\n",
    "multiplier = 0\n",
    "ax[0,2].set_xlim(0, 100)\n",
    "y_pos = np.arange(3)\n",
    "for attribute, measurement in notTNBCData.items():\n",
    "    offset = width * multiplier  \n",
    "    rectsNTNBC = ax[0,2].barh(y_pos + offset, measurement, width, label=attribute)\n",
    "    ax[0,2].bar_label(rectsNTNBC, padding=3, fmt='%.2f%%')\n",
    "    multiplier += 1\n",
    "\n",
    "ax[0,2].set_yticks(y_pos)\n",
    "ax[0,2].set_yticklabels(classLabels)\n",
    "ax[0,2].invert_yaxis()\n",
    "ax[0,2].set_xlabel('Metrics nTNBC')\n",
    "\n",
    "# == Confusion matrices\n",
    "# https://matplotlib.org/stable/gallery/images_contours_and_fields/image_annotated_heatmap.html\n",
    "\n",
    "N = 0\n",
    "\n",
    "for model in variants.keys():\n",
    "    x = N % ncols\n",
    "    y = (N // ncols) + 1\n",
    "    ax[y,x].imshow(confusionMatrixes[model], cmap='RdBu_r')\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            text = ax[y,x].text(j, i, confusionMatrixes[model][i, j], ha=\"center\", va=\"center\", color=\"white\")\n",
    "    \n",
    "    ax[y,x].yaxis.set_visible(False)\n",
    "    ax[y,x].set_xticklabels('')\n",
    "    ax[y,x].set_xlabel(f\"Confusion matrix - {model}\")\n",
    "    ax[y,x].tick_params(axis=u'both', which=u'both',length=0)\n",
    "    N += 1\n",
    "\n",
    "# Hide superfluous axes\n",
    "for i in range(3, ncols):\n",
    "    ax.flat[i].set_visible(False)\n",
    "for i in range(ncols + len(accuracyData), nrows * ncols):\n",
    "    ax.flat[i].set_visible(False)\n",
    "\n",
    "fig.legend(handles=rects, labels=models, bbox_to_anchor=(0.09, 0.7, 0.5, 0.5))\n",
    "\n",
    "chartsPartOne = plt.gcf()\n",
    "\n",
    "plt.savefig(fname=f\"../Data/lasso_validation_metrics.svg\", format='svg')\n",
    "plt.close()\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "rocAucData = {}\n",
    "fprTprData = {}\n",
    "\n",
    "# Read evaluation data for all files\n",
    "for model, file in variants.items():\n",
    "    dataPath = '../Data'\n",
    "    df = pd.read_csv(os.path.join(dataPath, file))\n",
    "    y_test = df['y_validation']\n",
    "    y_prob = df['y_prob']\n",
    "\n",
    "    fprTprData[model] = roc_curve(y_test, y_prob)\n",
    "    rocAucData[model] = auc(fprTprData[model][0], fprTprData[model][1]) * 100\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 1, ncols = 2, layout='constrained', figsize=[12,6])\n",
    "\n",
    "## Computing ROC curve [978-14-49-36988-0]\n",
    "\n",
    "\n",
    "for model, fprTpr in fprTprData.items():\n",
    "    ax[0].plot(fprTpr[0], fprTpr[1], label=model)\n",
    "\n",
    "rects = ax[1].barh(y=rocAucData.keys(), width=rocAucData.values(), color=colors)\n",
    "ax[1].set_xlim(0, 100)\n",
    "ax[1].bar_label(rects, padding=3, fmt='%.2f%%')\n",
    "ax[1].set_ylabel('AUC score')\n",
    "ax[1].set_yticklabels('')\n",
    "ax[1].invert_yaxis()\n",
    "\n",
    "fig.legend(handles=rects, labels=models, bbox_to_anchor=(0.09, 0.7, 0.5, 0.5))\n",
    "\n",
    "chartsPartTwo = plt.gcf()\n",
    "\n",
    "plt.savefig(fname=f\"../Data/lasso_validation_ROC.svg\", format='svg')\n",
    "plt.close()\n",
    "\n",
    "# Note: The actual N-Fold Cross-validation was performed by the model itself (see Model folder). Additional metrics besides accuracy were exported as well\n",
    "\n",
    "nFoldData = {}\n",
    "\n",
    "for model, file in variantMetrics.items():\n",
    "    dataPath = '../Data'\n",
    "    df = pd.read_csv(os.path.join(dataPath, file))\n",
    "    cs_scores = df['accuracy']\n",
    "    nFoldData[model] = cs_scores.mean() * 100\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "rects = ax.bar(nFoldData.keys(), nFoldData.values(), label=nFoldData.keys(), color=getColors(len(nFoldData)))\n",
    "ax.bar_label(rects, padding=3, fmt='%.2f%%')\n",
    "\n",
    "plt.savefig(fname=f\"../Data/lasso_validation_5fold.svg\", format='svg')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2ce59a5-888f-4add-8286-649c941f519a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
