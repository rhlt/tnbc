{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "776aca64-40b8-4873-bd0f-de8f8280e672",
   "metadata": {},
   "source": [
    "## Data helper functions (used by all notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61b1c2f2-26b3-4030-98d6-566abb7b7f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "def split_data(df, target, case_id=None):\n",
    "    \n",
    "    # Features: all columns except target column\n",
    "    X = df.drop(columns=[target])\n",
    "    # Target variable\n",
    "    y = df[target]\n",
    "\n",
    "    return capstone_train_test_split(X, y, case_id)\n",
    "\n",
    "def split_data_apply_smote(df, target, case_id=None):\n",
    "\n",
    "    # Features: all columns except target column\n",
    "    X = df.drop(columns=[target, 'case_id']) # SMOTE cannot work with string / guid, case_id drop\n",
    "    # Target variable\n",
    "    y = df[target]\n",
    "\n",
    "    sm = SMOTE(random_state=42) # can have different parameters\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "    return capstone_train_test_split(X_res, y_res, case_id)\n",
    "\n",
    "def capstone_train_test_split(X, y, case_id):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "    \n",
    "    # Take out case ID but keep then available for testing data (for initial validation)\n",
    "    if case_id is not None:\n",
    "        test_case_id = X_test[case_id]\n",
    "        X.drop(columns=[case_id], inplace=True)\n",
    "        X_train.drop(columns=[case_id], inplace=True)\n",
    "        X_test.drop(columns=[case_id], inplace=True)\n",
    "    else:\n",
    "        test_case_id = None\n",
    "    \n",
    "    # Training size = 0.8 * 977 ≈ 781\n",
    "    # Test size = 0.2 * 977 ≈ 196\n",
    "    print(f\"{X_train.shape=}\")\n",
    "    print(f\"{X_test.shape=}\")\n",
    "    print(f\"{y_train.shape=}\")\n",
    "    print(f\"{y_test.shape=}\")\n",
    "\n",
    "    return X, y, X_train, X_test, y_train, y_test, test_case_id\n",
    "\n",
    "\n",
    "def get_metrics(y_true, y_pred, y_prob=None):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"f1_score\": f1_score(y_true, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_true, y_prob) if y_prob is not None else None,\n",
    "        \"true_positive\": tp,\n",
    "        \"true_negative\": tn,\n",
    "        \"false_positive\": fp,\n",
    "        \"false_negative\": fn,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def get_cross_validation_metrics(model, X, y, cv = 5):\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n",
    "    results = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "        X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred_fold = model.predict(X_val_fold)\n",
    "        y_prob_fold = model.predict_proba(X_val_fold)[:, 1]\n",
    "        \n",
    "        metrics = get_metrics(y_val_fold, y_pred_fold, y_prob_fold)\n",
    "        metrics[\"fold\"] = fold + 1 # ID 0 will be used for the initial testing data\n",
    "        results.append(metrics)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.set_index(\"fold\", inplace=True)\n",
    "    return df\n",
    "\n",
    "def print_evaluated_model_accuracy(y_test, y_pred):\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")    \n",
    "\n",
    "def print_validated_model_accuracy(model, metrics):\n",
    "    print(f\"Model validation for {type(model).__name__}:\")\n",
    "    accuracy = metrics[\"accuracy\"]\n",
    "    print(accuracy.to_list())\n",
    "    print(f\"\\nMean accuracy: {accuracy.mean():.4f}\\n\")\n",
    "    return metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
