{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "776aca64-40b8-4873-bd0f-de8f8280e672",
   "metadata": {},
   "source": [
    "## Data helper functions (used by all notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b1c2f2-26b3-4030-98d6-566abb7b7f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from pandas import DataFrame, Series\n",
    "from enum import StrEnum, auto\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "TrainTestData = tuple[DataFrame, Series, DataFrame, DataFrame, Series, Series]\n",
    "Model = LogisticRegression | SVC | RandomForestClassifier\n",
    "\n",
    "class FeatureVariant(StrEnum):\n",
    "    RESEARCH_MANUAL = 'research_ManualAssessment'\n",
    "    RESEARCH_BORUTA = 'research_BORUTA'\n",
    "    RESEARCH_TREE = 'research_extraTrees'\n",
    "    RESEARCH_LOG = 'research_log'\n",
    "    RESEARCH_LOGSCALED = 'research_logScaled'\n",
    "    RESEARCH_ANOVA = 'research_kBestANOVA'\n",
    "    LITERATURE = auto()\n",
    "    STATISTICAL = auto()\n",
    "    AUTOMATED = auto()\n",
    "\n",
    "    def print_info():\n",
    "        print([key for key in FeatureVariant.__members__])\n",
    "\n",
    "class ModelVariant(StrEnum):\n",
    "    SVM = 'svm'\n",
    "    RF = 'random_forest'\n",
    "    LG = 'logistic_regression'\n",
    "\n",
    "    def print_info():\n",
    "        print([key for key in ModelVariant.__members__])\n",
    "\n",
    "\n",
    "def split_data(df: DataFrame, target: str, case_id=None) -> TrainTestData:\n",
    "\n",
    "    # Features: all columns except target column\n",
    "    X = df.drop(columns=[target])\n",
    "    # Target variable\n",
    "    y = df[target]\n",
    "\n",
    "    return capstone_train_test_split(X, y, case_id)\n",
    "\n",
    "def split_data_apply_smote(df: DataFrame, target: str) -> TrainTestData:\n",
    "\n",
    "    # Features: all columns except target column\n",
    "    X: DataFrame = df.drop(columns=[target, 'case_id']) # SMOTE cannot work with string / guid, case_id drop\n",
    "    # Target variable\n",
    "    y: Series = df[target]\n",
    "\n",
    "    sm = SMOTE(random_state=42) # can have different parameters\n",
    "    X_res, y_res = sm.fit_resample(X, y)\n",
    "\n",
    "    return capstone_train_test_split(X_res, y_res)\n",
    "\n",
    "def capstone_train_test_split(X: DataFrame, y: Series, contains_case_id: bool = False) -> TrainTestData:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "    # Take out case ID but keep then available for testing data (for initial validation)\n",
    "    if contains_case_id:\n",
    "        case_id: str = \"case_id\"\n",
    "        test_case_id: Series = X_test[case_id]\n",
    "        X.drop(columns=[case_id], inplace=True)\n",
    "        X_train.drop(columns=[case_id], inplace=True)\n",
    "        X_test.drop(columns=[case_id], inplace=True)\n",
    "    else:\n",
    "        test_case_id = None\n",
    "\n",
    "    # Training size = 0.8 * 977 ≈ 781\n",
    "    # Test size = 0.2 * 977 ≈ 196\n",
    "    print(f\"{X_train.shape=}\")\n",
    "    print(f\"{X_test.shape=}\")\n",
    "    print(f\"{y_train.shape=}\")\n",
    "    print(f\"{y_test.shape=}\")\n",
    "\n",
    "    return X, y, X_train, X_test, y_train, y_test, test_case_id\n",
    "\n",
    "\n",
    "def get_metrics(y_true: Series, y_pred: Series, y_prob=None) -> dict[str, float | int]:\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    metrics: dict[str, float | int] = {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"recall\": recall_score(y_true, y_pred),\n",
    "        \"precision\": precision_score(y_true, y_pred),\n",
    "        \"f1_score\": f1_score(y_true, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_true, y_prob) if y_prob is not None else None,\n",
    "        \"true_positive\": tp,\n",
    "        \"true_negative\": tn,\n",
    "        \"false_positive\": fp,\n",
    "        \"false_negative\": fn,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def get_cross_validation_metrics(model: Model, X: DataFrame, y: Series, cv: int = 5) -> DataFrame:\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=RANDOM_STATE)\n",
    "    results = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "        X_train_fold, X_val_fold = X.iloc[train_index], X.iloc[val_index]\n",
    "        y_train_fold, y_val_fold = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        y_pred_fold = model.predict(X_val_fold)\n",
    "        y_prob_fold = model.predict_proba(X_val_fold)[:, 1]\n",
    "        \n",
    "        metrics: dict[str, float | int] = get_metrics(y_val_fold, y_pred_fold, y_prob_fold)\n",
    "        metrics[\"fold\"] = fold + 1 # ID 0 will be used for the initial testing data\n",
    "        results.append(metrics)\n",
    "\n",
    "    df = DataFrame(results)\n",
    "    df.set_index(\"fold\", inplace=True)\n",
    "    return df\n",
    "\n",
    "def print_evaluated_model_accuracy(y_test: Series, y_pred: Series) -> None:\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")    \n",
    "\n",
    "def print_validated_model_accuracy(model: Model, metrics: DataFrame) -> DataFrame:\n",
    "    print(f\"Model validation for {type(model).__name__}:\")\n",
    "    accuracy = metrics[\"accuracy\"]\n",
    "    print(accuracy.to_list())\n",
    "    print(f\"\\nMean accuracy: {accuracy.mean():.4f}\\n\")\n",
    "    return metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
