# 4. Implementations of Machine Learning models

***Prerequisite:** Files with selected features named **patient_genes_[variant].csv** must be generated as described in **[Features](../Features)**.*

These notebooks contain the initial setup of the Machine Learning models based on the following algorithms:

- Logistic Regression
- Support Vector Machines (SVM)
- Random Forest

The dataset used is *patient_genes_literature.csv*, though this can easily be swapped with a different feature set.
General data helper functions are declared in the DataHelpers notebook. This includes splitting the dataset into 80% training data and 20% testing data, as well as functionality for K-fold cross validation and the exporting of various evaluation metrics. All splits are stratified to account for the imbalance present in the dataset.

The notebooks for the respective models train the type of model they are named after (as listed above) and use the helper functions to perform the actual data splitting, five-fold cross-validation, and preliminary evaluation.

A summarization of the cross-validation results is displayed at the end of the notebook, and all necessary data is exported to the following files, where *[variant]* refers to the model type trained:

- **model_output_[variant].csv**, containing the output for each test case selected from the dataset: the actual TNBC value, the predicted value, and the predicted probability
- **model_metrics_[variant].csv**, containing the accuracy, recall, precision, F1-score, ROC/AUC, and the numbers of True Positives, True Negatives, False Positives and False Negatives, for both the initial test data (first row) and the five folds used in cross-validation.


***Next step***: Additional evaluations and visualization based on the generated data, including combining those for different models, is generated by the notebook in the **[Evaluation](../Evaluation)** folder, that lives next to this Model folder.

## Key findings

When using literature-based features based on the key findings of the previous step, the three models trained (logistic regression, random forest and SVM) perform very similarly. Overall, performance is moderate or at best slightly better than moderate, which is likely caused by the limited size and relative imbalance of the dataset, with only a small number of positive TNBC samples.

Interestingly, perfect recall is seen on the initial split. However, this disappears in cross-validation as well as when changing the initial random state. This seems to be an artifact of the random state and limited positive sample size, rather than model quality or training issues.

While the models perform similarly, logistic regression shows slightly weaker performance compared to the others, particularly in precision and F1.

## Suggestions for improvement ***[TODO IF TIME PERMITS]***
- Apply SMOTE to remove the imbalance **(moet hier want SMOTE moet alleen op de testdata, dus na de split!)**
- Train more (different types of) models, e.g. XGBoost
