# 4. Implementations of Machine Learning models

***Prerequisite:** Files with selected features named **patient_genes_[variant].csv** must be generated as described in **[Features](../Features)**.*

These notebooks contain the initial setup of the Machine Learning models based on the following algorithms:

- Logistic Regression
- Support Vector Machines (SVM)
- Random Forest

The dataset used is *patient_genes_literature.csv*, though this can easily be swapped with a different feature set.
General data helper functions are declared in the DataHelpers notebook. This includes splitting the dataset into 80% training data and 20% testing data, as well as functionality for K-fold cross valid ation and the exporting of various evaluation metrics. All splits are stratified to account for the imbalance present in the dataset.

The notebooks for the respective models train the type of model they are named after (as listed above) and use the helper functions to perform the actual data splitting, five-fold cross-validation, and preliminary evaluation.

A summarization of the cross-validation results is displayed at the end of the notebook, and all necessary data is exported to the following files, where *[variant]* refers to the model type trained:

- **model_output_[variant].csv**, containing the output for each test case selected from the dataset: the actual TNBC value, the predicted value, and the predicted probability
- **model_metrics_[variant].csv**, containing the accuracy, recall, precision, F1-score, ROC/AUC, and the numbers of True Positives, True Negatives, False Positives and False Negatives, for both the initial test data (first row) and the five folds used in cross-validation.

***Next step***: Additional evaluations and visualization based on the generated data, including combining those for different models, is generated by the notebook in the **[Evaluation](../Evaluation)** folder, that lives next to this Model folder.

## Key findings

The three models trained (logistic regression, random forest and SVM) perform very similarly. Overall, performance is moderate, which is likely caused by the limited size and relative imbalance of the dataset, with only a small number of positive TNBC samples.

Interestingly, perfect recall is seen on the initial split. However, this disappears in cross-validation as well as when changing the initial random state. This seems to be an artifact of the random state and limited positive sample size, rather than model quality or training issues.

While the models perform similarly, logistic regression shows slightly weaker performance compared to the others, particularly in precision and F1.

### After applying SMOTE
It seems that SMOTE improves or stabilizes performance in most cases, particularly for Random Forest.

Comparing the models performance, we can see that Random Forest consistently outperforms Logistic Regression and SVM, especially after SMOTE.

**TODO**
Victor: Explain SVM -> Accuracy drops after SMOTE for Literature features.
What does this mean?

#### Breakdown by model

**Logistic Regression**

| Feature Set | SMOTE | Accuracy       | MEAN CV Accuracy  |
|-------------|-------|----------------|-------------------|
| Literature  | NO    | 0.93           | 0.9437            |
| Literature  | YES   | 0.94           | 0.9385:arrow_down:|
| Research    | NO    | 0.93           | 0.9273            |
| Research    | YES   | 0.94 :arrow_up:| 0.9333:arrow_down:|

**Findings**:

- Literature features perform slightly better in CV without SMOTE.

**TODO** Victor: Explain how it's possible that CV performance is worsened

- SMOTE helps a little with Research features but slightly worsens Literature CV performance.

- Overall, SMOTE provide modest benefit.

**SVM**

| Feature Set | SMOTE | Accuracy         | MEAN CV Accuracy |
|-------------|-------|------------------|------------------|
| Literature  | NO    | 0.96             | 0.9263           |
| Literature  | YES   | 0.94 :arrow_down:| 0.9432 :arrow_up:|
| Research    | NO    | 0.94             | 0.9273           |
| Research    | YES   | 0.94             | 0.9321 :arrow_up:|

**Findings**:

**TODO**
Victor: Accuracy drops after SMOTE for Literature features.
What does this mean?
- Accuracy drops slightly after SMOTE on Literature features, but CV improves

**TODO**
Victor: Accuracy drops after SMOTE for Literature features.
What does this mean?
- Identical scores between Literature and Research without SMOTE.

- SVM benefits moderately from SMOTE, especially in CV.

**Random Forest**

| Feature Set | SMOTE | Accuracy       | MEAN CV Accuracy |
|-------------|-------|----------------|------------------|
| Literature  | NO    | 0.94           | 0.9396           |
| Literature  | YES   | 0.96 :arrow_up:| 0.9635 :arrow_up:|
| Research    | NO    | 0.95           | 0.9396           |
| Research    | YES   | 0.96 :arrow_up:| 0.9698 :arrow_up:|

**Findings**:

- Clear indication that SMOTE helps Random Forest across both feature sets.
- Research features with SMOTE yield the best cross-validation score compared with other models (0.9698).
- Suggests that Random Forest is robust and can take advantage of synthetic minority samples effectively.

**TODO**
Victor:
Provide interpretations of the feature sets
Suggestions for improvents

## Suggestions for improvement ***[TODO IF TIME PERMITS]***
- ~~Apply SMOTE to remove the imbalance **(moet hier want SMOTE moet alleen op de testdata, dus na de split!)**~~
- Train more (different types of) models, e.g. XGBoost
